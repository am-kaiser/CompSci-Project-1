\section{Summary} \label{sec:summary}

We trained various linear regression models like OLS, ridge, and lasso to fit polynomials to the Franke function with noise. Thereafter, we used the model to predict the output for unseen test data. Various statistics like $MSE$, $R^2$ Error, and generalization error were used to probe the model performance. We also applied resampling techniques to reduce the uncertainty in the statistics. Along with finding optimal parameters directly using closed-form solutions, we also utilized stochastic gradient descent to find optimal parameters. We discovered that OLS is better at predicting than lasso and ridge for the sample data considered. When the lasso and ridge models tend to more OLS like behavior i.e. $\lambda_l$ and $\lambda_r$ tend to 0, the prediction improves. OLS performed several orders of magnitude better than lasso regression and a few orders of magnitude better than ridge in terms of the prediction error. We also experienced the problem of overfitting when the training data was less numerous and particularly during bootstrap resampling where the effective data that the model could 'see' during the training phase was reduced due to sampling with replacement. \newline
In addition, we used logistic regression and Support Vector Machines to fit a model to the Wisconsin Breast Cancer data. Overall, we showed that SVM with a linear kernel is the best performing classification algorithm for this setup. Nonetheless, all tested methods performed well when classifying benign classes. While the performance could be improved for malignant ones.