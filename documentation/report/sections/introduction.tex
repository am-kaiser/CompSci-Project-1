\section{Introduction} \label{sec:introduction}

The focus of this report is to discuss some commonly used supervised learning techniques. \newline \newline
We start with introducing the linear regression model. It is a model that predicts a response given an input vector and introduces a learnable parameter that is linear with respect to the input variable. Even though it is called linear, it can be used to model nonlinear functions. A combination of fixed nonlinear functions of input variables serves as a basis function to extend the linear regression capability. We use linear regression to fit 2D polynomials to the noisy Franke function. To obtain optimal model parameters, we use two common techniques. In the first technique the optimal parameters are obtained by minimizing the mean-squared error for which we have an analytic expression. This can be computationally costly since it involves calculating the matrix inverse. The second approach, commonly used for large data sets, uses stochastic gradient descent. We also explore both methods using shrinkage methods with L1 and L2 regularization. In addition, we use two resampling techniques, bootstrap resampling and cross-validation, to improve the reliability of the model performance statistics. Ultimately, we aim to obtain a statistical model that can map a new input variable to a response with acceptable accuracy. \newline \newline
Another type of supervised learning is classification. Here, we explore logistic regression and support vector machines. We use the Wisconsin breast cancer data. The optimization or training uses stochastic gradient descent with mini-batches for logistic regression. The classification models were trained on the cancer data using various hyperparameters such as minibatch size, number of epochs, and learning rate. For classification of the continuous-valued output, logistic regression uses a decision boundary. In the case of SVM classification, we utilize the built-in functionality of scikit. \newline \newline
For each type of supervised learning in the following section, we start with a theoretical discussion, followed by results and a discussion of them. \newline \newline
The code can be found in the \href{https://github.com/am-kaiser/CompSci-Project-1}{GitHub repository}.
