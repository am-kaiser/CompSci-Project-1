\subsection{Method} \label{subsec:method_linear_regression}

Consider that we have a data set of inputs and their corresponding output. Regression is a process of arriving at a relation between the independent input variables and the dependent output variables. In the field of statistical learning, regression is considered to be a supervised learning method. This is because the model parameters generated in regression are learned from labeled data which is data that comprises both inputs and their corresponding output, i.e the labels. In this report, we assume that there are multiple input variables and a single output variable for each member of the data set.
\newline \newline
The dataset with $n$ members is defined as $\mathcal{D} = [({x}_1, y_1 ), ({x}_2, y_2 ), ..., ({x}_n, y_n )]$. Where ${x}_i$ and $y_i$  are the $i^{th}$ input and output respectively. Let, $m$ be the number of components of the input such that ${x}_i := (x_{i1}, x_{i2}, ..., x_{im})$. In linear regression we assume that the data is of the form
\begin{equation}
    {y}_i = 1 \cdot\beta_0 + x_{i1}\beta_1 + x_{i2}\beta_2 + ... + x_{im}\beta_m + \epsilon_i \label{eq:lin_reg_data}
\end{equation}
where $\beta_0, \beta_1, \dots, \beta_m$ are unknown fixed regression parameters and $\epsilon_i$ is a random error or noise process consisting of independent identically (iid) normally distributed variables with mean zero and variance $\sigma^2$ \cite{shumway_time_2017}. Equation \ref{eq:lin_reg_data} can be rewritten as $y=\beta \phi(x) + \epsilon$ with
\begin{equation}
    \phi(x):=[1, x_{i,1}, x_{i,2}, \dots, x_{i,m}]
\end{equation}
Moreover, it can be shown that $E[y] = \beta \phi(x)$ and $var(y) = \sigma^2$. Therefore, $y \sim \mathcal{N}(\beta \phi(x),\,\sigma^{2})$ \cite{murphy_machine_2012}.
\newline \newline
In linear regression, we seek to fit a best-fit line between input, or some transformation of the input, and the output. This best-fit line is chosen such that it minimizes an error or a cost function. The best fit line is a function of input variables and its value is called the predicted value or the fit. We denote this with $\hat{y}$. The linear regression model has the form:
\begin{equation}
    \hat{y}_i = 1 \cdot\beta_0 + x_{i1}\beta_1 + x_{i2}\beta_2 + ... + x_{im}\beta_m
\end{equation}
The $\beta$'s are chosen such that they minimize a cost function. If this cost function chosen to be the mean squared error (MSE) then we call the regression \textit{least squares} regression. The mean squared error is defined as 
\begin{equation}
    \mathcal{C} = \frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2 \label{eq:cost_function_sum}
\end{equation}

By extending the input such that ${x}_i = (1_i, x_{i1}, x_{i2}, ..., x_{im})$, i.e. the first entry of $x_i$ is a vector which only contains 1, and denoting the parameter vector by ${\beta} = (\beta_0, \beta_1, ..., \beta_m)$, we can express the predicted value as an inner product such that   
\begin{equation*}
    \hat{y}_i = {x_i}\cdot {\beta} = {x_i}^T{\beta}.
\end{equation*}
Moreover, we can express the set of predicted outputs as a vector which allows us to write the previous equation as a matrix-vector product:
\begin{equation}
    \hat{y} = {X}{\beta} \label{eq:lin_reg_matrix_model}
\end{equation}
where, $\hat{y}$, ${X}$ and ${\beta}$ have dimensions $n \times 1$, $n \times (m+1)$ and $(m+1) \times 1$.

However, such a description is only useful if the output is a linear or near-linear function of the input variables.  We can make the model more "complex" by allowing higher powers of input variables and their products to be the features. One possibility is to consider polynomial functions of the input variables. For simplicity, we consider input vectors where $m=2$ as they are the only ones relevant for this project. Then, we can express the predicted output $\hat{y}$ as a polynomial function of order $p$ such that
\begin{equation}
    \hat{y}_i = \beta_0 + \sum_{k=1}^{k=p}\sum_{l=0}^{l=p}\beta_{\frac{k(k+1)}{2}+l}x_{i1}^{p-l}x_{i2}^{l}
\end{equation}
which is equivalent to
\begin{equation}
\resizebox{.9\hsize}{!}{$
    \underbrace{\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{bmatrix}}_{n \times 1} = \underbrace{\begin{bmatrix} 
    1 & x_{11} & x_{12} & x_{11}^2 & \dots & x_{11}^p & x_{11}^{p-1}x_{12} & \dots & x_{11}x_{12}^{p-1} & x_{12}^p 
    \\ 1 & x_{21} & x_{22} & x_{21}^2 & \dots & x_{21}^p & x_{21}^{p-1}x_{22} & \dots & x_{21}x_{22}^{p-1} & x_{22}^p 
    \\ \vdots &&&&& \ddots &&&&\vdots \\ 
    1 & x_{n1} & x_{n2} & x_{n1}^2 & \dots & x_{n1}^p & x_{n1}^{p-1}x_{n2} & ... & x_{n1}x_{n2}^{p-1} & x_{n2}^p
    \end{bmatrix}}_{n \times \frac{p^2+5p}{2}}\underbrace{\begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_{\frac{p^2+3p}{2}}\end{bmatrix}.}_{\frac{p^2+5p}{2} \times 1}$}
\end{equation}
For clarity if $p=2$
\begin{equation*}
    \hat{y}_i = \beta_0+\beta_1 x_{i1} + \beta_2 x_{i2}.
\end{equation*}
We see that the output vector is linear in the vector of parameters even though it is non linear in the features. Note that $p$ determines the model complexity and therefore the number of columns in the design matrix and the number of parameters. The output vector though depends only on the number of inputs. By changing $p$ we are changing the basis with which we describe $y_i$. 
\newline \newline
As mentioned before our aim is to find $\beta$ such that the cost function is minimized. We start by rewriting equation \ref{eq:cost_function_sum}:
\begin{equation}
    \mathcal{C}(\beta) = \frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i(\beta))^2 = \frac{1}{n}( y -  {\hat y})^T( y -  {\hat y}) = \frac{1}{n}( y -  {X\beta})^T( y -  {X\beta}).
\end{equation}

The cost function is minimum when its gradient with respect to all parameters is 0, i.e.
\begin{equation*}
    \nabla_{\beta}\mathcal{C}(\beta) = X^T(y-X\beta) = 0.
\end{equation*}

Assuming that $X$ has full column rank, and hence $X^T X$ is positive definite,solving the above equation provides us with the optimal parameters as follows
\begin{equation}
    \beta = (X^TX)^{-1}X^Ty.
\end{equation}
The cost function for a linear regression problem is convex. Hence, there is only one global extremum. This is not proven in this report.
\newline \newline
In the case where $X^T X$ is near singular or singular calculating pseudoinverse (specifically the
Moore-Penrose inverse) is very useful and a well known method. It calculates the inverse by using the Singular Value Decomposition algorithm. Moreover, this approach is more efficient than computing the regular inverse \cite{geron_hands-machine_2019}.
\newline \newline
An important step when calculating the best suited model for given in- and output data is the splitting of the data in a training and a test data set. The test data set provides an unbiased evaluation of the fit of the model, which was trained on the training data set. \newline
The difference in the error values  for the training and testing set determines the degree of generalization of the model from trained data to new unseen data. The average generalisation error is \cite{mehta_high-bias_2019}
\begin{equation*}
    |MSE_{train} - MSE_{test}| = 2\sigma^2\frac{m}{n}.
\end{equation*}
If $m >> n$ then the model is not generalising, i.e. learning. In addition, the error can be large if the intrinsic noise $\sigma^2$ in the data is large. To counter this problem, regularisation is performed. In regularisation, the parameter values are penalised. In ridge regression, the $L^2$ norm of the parameter vector is penalised and in Lasso regression, the $L^1$ norm of the parameter vector is penalised. This is done by modifying the cost functions as follows
\begin{align}
    \mathcal{C}_{ridge}(\beta) &=  \frac{1}{n}( y -  {X\beta})^T( y -  {X\beta}) + \lambda||\beta||_2^2 \\
    \mathcal{C}_{lasso}(\beta) &=  \frac{1}{n}( y -  {X\beta})^T( y -  {X\beta}) + \lambda||\beta||_1
\end{align}

\subsubsection{Feature Scaling}\label{subsubsec:scaling}
In general, machine learning algorithms do not perform well if the scales of the data vary a lot. There exist different scaling methods. We focus on min-max scaling, also known as min-max normalization. In this approach the values are scaled and shifted such that they range from 0 to 1, i.e. 
\begin{equation*}
    x^{'} = \frac{x - min(x)}{max(x) - min(x)}.
\end{equation*}

Typically, data contains outliers. These outliers can be erroneously obtained data. They can also be the true data that sometimes deviates from the established patterns of the system due to intrinsic noise. The presence of outliers in both the training and the testing set can either influence the fitting of parameters or the statistics. Therefore, it is essential to subdue the effects of outliers using methods that usually involve multiple iterations of training and testing on different subsets of the data. Resampling techniques are commonly used in this case. In general, resampling refers to repeatedly drawing a sample from the training data set and refitting the model on every sample. In the end, the mean of the statistics of individual iterations is taken to be the final statistic. \newline \newline In the following sections we discuss two of the most commonly used methods: the bootstrap algorithm and cross-validation. 
\subsubsection{Bootstrap Algorithm}\label{subsubsec:bootstrap}
There are multiple variants of the bootstrap method. We use one that is distinct from the lecture notes and we also motivate our reasons behind this. 

Bootstrap algorithm:
\begin{enumerate}
    \item Let the number of bootstrap iterations on given data be: $n_{boots}$
    \item For each bootstrap iteration $n_{bi}$:
    \begin{enumerate}
        \item The input data consisting of $n$ samples is randomly divided into two categories. The two categories are the training and test set. The training set has $(1-r)n$  number of samples which are randomly selected with replacement. The rest of the samples then make up the testing set. Therefore the number of samples is the testing set is greater than or equal to $rn$. Here, $r$ is the testing ratio.
        \item Train parameters for the training set and fit them to the testing set.
        \item Compute statistics on the training set: $MSE_i, R^2_i$
    \end{enumerate}
    \item The final statistics of the bootstrap algorithm are the mean of the statistics of all the bootstrap iterations.
\end{enumerate}
\subsubsection{Cross-Validation}\label{subsubsec:cross_validation}
\begin{enumerate}
    \item The data is divided into a finite number $k_{folds}$ of equal-sized sets called folds
    \item For each fold $k_{fi}$,
    \begin{enumerate}
        \item The fold is taken as the testing set while the rest of the folds comprise the training set.
        \item Train parameters for the training set and fit them to the testing set.
        \item Compute statistics on the training set: $MSE_i, R^2_i$
    \end{enumerate}
    \item The final statistics of the cross-validation algorithm are the mean of the statistics obtained by considering each fold as the testing set.
\end{enumerate}

\subsubsection{Stochastic Gradient Descent}\label{subsubsec:SGD}
SGD is not a machine learning technique but an optimization technique. It is useful in the optimization requirement for logistic regression. 

SGD comes in various variants. The most common variant due to its good empirical performance is a mix of epoch and mini-batches. Mini-batches are a subset of data of size m obtained by sampling the whole training data without replacement. This is where randomness and stochasticity come from.